% !TeX spellcheck = sv_SE
\documentclass[a4paper, 12pt]{report}

\usepackage[swedish]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}

\usepackage{graphicx}

\DeclareMathOperator{\sign}{sign}

\theoremstyle{definition}
\newtheorem{thm}{Sats}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Korollarium}
\newtheorem{defi}{Definition}[section]
\newtheorem{ex}{Exempel}[section]
\theoremstyle{remark}
\newtheorem*{rem}{Observation}

\newtheorem*{reas}{Orsak}

\newcommand{\bfbeta}{{\boldsymbol{\beta}}}
\renewcommand\qedsymbol{$\blacksquare$}

\newcommand{\sephyp}{\{ \mathbf{x} : f(\mathbf{x})=\mathbf{x}^\intercal \bfbeta + \beta_0=0\}}

\newcommand{\entsephyp}{\{ \mathbf{x} : f(\mathbf{x})=\mathbf{x}^\intercal \bfbeta + \beta_0=0,~y_if(\mathbf{x}_i)\geq0,~i=1,\dots,~N,~\|\bfbeta\|=1\}}
\title{Hilbertrum med Reproducerande Kärnor}
\author{Oscar Granlund}


\begin{document}
\maketitle

\begin{abstract}
	Testtesttesttesttesttest
\end{abstract}

\chapter{Stödvektormaskiner (SVM)}

\section{Klassificering med hjälp av separerande hyperplan}

INTRODUKTION OM VARFÖR KLASSIFICERING, EXEMPEL MED SPAM-FILTER

\begin{defi}
	Ett \textit{klassificeringsproblem} är ett problem var man utgående från en mängd observationspar (\textit{träningsdata}) $(\mathbf{x}_i,y_i)$, $\mathbf{x}_i\in\mathbb{R}^p$, $y_i\in \{-1,1\}$, $i=1,\dots,~N$, försöker hitta en regel $g: \mathbb{R}^p \longmapsto \{-1,1\}$ sådan att $g(\mathbf{x}_i)=y_i$ för alla träningspar $(\mathbf{x}_i,y_i)$.
\end{defi}

Inom statistiken och maskininlärningen finns många olika metoder för att försöka lösa klassificeringsproblem, till exempel med hjälp av regression eller någon sorts klusteralgoritm. I detta kapitel behandlas en metod där affina mängder med dimensionerna $p-1$ används för att definiera en regel som klassificerar \textit{observationerna} $\mathbf{x}_i$ i \textit{klasserna} $y_i\in\{-1,~1\}$ genom separering.

\begin{defi}
	Ett \textit{hyperplan} i ett vektorrum med dimensionen $p$ är ett underrum med dimensionen $p-1$; figur \ref{fig:separatinghyperplane} illustrerar ett separerande hyperplan för fallet $p=2$. Klassificeringsregeln $g$ för separerande hyperplan blir $g(\mathbf{x}_i)=\sign (\mathbf{x}_i^\intercal    \bfbeta + \beta_0)$ där mängden $\{\mathbf{x}: \mathbf{x}^\intercal \bfbeta + \beta_0=0\}$, med $\mathbf{x},~\bfbeta\in \mathbb{R}^p$, definierar ett hyperplan alternativt en \textit{affin} mängd, parametriserat av $\bfbeta$ och $\beta_0$.
\end{defi}

\begin{thm}\label{thm:hyperplan}
	Ett hyperplan definierat som den affina mängden $L=\sephyp$ har följande egenskaper \cite{ESL}:
	\begin{enumerate}
		\item Den normaliserade normalvektorn $\widehat{\bfbeta}_\mathrm{n}$ kan skrivas på formen
		\begin{equation*}
			\widehat{\bfbeta}_\mathrm{n} = \frac{\bfbeta}{\|\bfbeta\|}.
		\end{equation*}
		\item $\mathbf{x}_0^\intercal \bfbeta = -\beta_0$ för alla $\mathbf{x}_0$ i $L$.
		\item Det signerade avståndet från en punkt $\mathbf{x}$ till hyperplanet $L$ ges av
		\begin{equation*}
		\begin{aligned}
			(\mathbf{x}-\mathbf{x}_0)^\intercal \widehat{\bfbeta}_\mathrm{n} &= \frac{1}{\|\bfbeta\|}(\mathbf{x}^\intercal \bfbeta+\beta_0)\\
			&= \frac{1}{\|f^\prime(\mathbf{x})\|}f(\mathbf{x}).
		\end{aligned}
		\end{equation*}
	\end{enumerate}
\end{thm}
\begin{proof}
	\leavevmode
\begin{enumerate}
	\item Låt $\mathbf{x}_1$ och $\mathbf{x}_2$ vara två punkter i $L$. Då gäller att $f(\mathbf{x}_1)=f(\mathbf{x}_2)=0$ och
	\begin{align*}
		0 &= f(\mathbf{x}_1)-f(\mathbf{x}_2)\\
		&= \mathbf{x}_1^\intercal \bfbeta + \beta_0 - \mathbf{x}_2^ \intercal \bfbeta - \beta_0\\
		&= (\mathbf{x}_1-\mathbf{x}_2)^\intercal \bfbeta
	\end{align*}
	alltså uppfyller $\bfbeta$ kravet för normalvektorer och $\widehat{\bfbeta}_\mathrm{n}:=\frac{\bfbeta}{\|\bfbeta\|}$ är den normaliserade normalvektorn till hyperplanet $L$. \hfill\qedsymbol
	\item Låt $\mathbf{x}_0$ vara en punkt i $L$. Då gäller att $f(\mathbf{x}_0)=\mathbf{x}_0^\intercal \bfbeta + \beta_0 = 0$ alltså är $\mathbf{x}^\intercal \bfbeta = - \beta_0$.\hfill \qedsymbol
	\item Låt $\mathbf{x}_0$ vara en punkt i hyperplanet $L$. Då är avståndet från hyperplanet till punkten $\mathbf{x}$ lika med längden av projektionen av vektorn $(\mathbf{x}-\mathbf{x}_0)$ på hyperplanets normal, $\beta$. Vi får alltså att
	\begin{align*}
		\operatorname{d^\pm} ( \mathbf{x}, L ) &= \operatorname{comp_{\bfbeta}} ( \mathbf{x} - \mathbf{x}_0 )
		=\underline{\underline{ (\mathbf{x} - \mathbf{x}_0)^\intercal \widehat{\bfbeta} }}\\
		&= \frac{1}{\|\bfbeta\|}(\mathbf{x}^\intercal\bfbeta - \mathbf{x}_0^\intercal\bfbeta)=\underline{\underline{\frac{1}{\|\bfbeta\|}(\mathbf{x}^\intercal\bfbeta + \bfbeta_0)}}
	\end{align*}
	och om man noterar att $f(\mathbf{x})=\mathbf{x}^\intercal\bfbeta+\beta_0$ och $f^\prime(\mathbf{x})=\bfbeta$ så fås även att
	\begin{equation*}
		\frac{1}{\|\bfbeta\|}(\mathbf{x}^\intercal\bfbeta + \bfbeta_0)=\frac{1}{\|f^\prime(\mathbf{x})\|}f(\mathbf{x}).
	\end{equation*}
	\qedhere
\end{enumerate}
\end{proof}

\begin{rem}
	Definitionen för hyperplanet $L=\{ \mathbf{x} : f(\mathbf{x})=\mathbf{x}^\intercal \bfbeta + \beta_0=0\}$ är inte entydig.
\end{rem}
\begin{reas}
	Betrakta hyperplanen $L_1 = \sephyp$ och $L_2 = \{\mathbf{x}: g(\mathbf{x})=\mathbf{x}^\intercal(-1\cdot\bfbeta) + (-1 \cdot \beta_0)\}$. Eftersom att $g(\mathbf{x}) = -f(\mathbf{x})$ så gäller att om $\mathbf{x}$ tillhör $L_1$ så tillhör $\mathbf{x}$ även $L_2$.
	Betrakta vidare $L_3= \{\mathbf{x}: h(\mathbf{x})=\frac{\mathbf{x}^\intercal\bfbeta}{\|\bfbeta\|} + \frac{\beta_0}{\|\bfbeta\|}\}=0$. Om $\mathbf{x}$ då tillhör $L_1$ så tillhör $\mathbf{x}$ även $L_3$ eftersom att $h(\mathbf{x}) = \frac{f(\mathbf{x})}{\|\bfbeta\|}=0$. Notera även att $\|\bfbeta\|$ kunde ha varit vilket reellt tal $\alpha$ som helst.
\end{reas}

\begin{rem}
	För att få entydiga hyperplan för klassificering kan man lägga till villkor. Om man kräver att $\|\bfbeta\|=1$ och $y_i(\mathbf{x}_i^\intercal\bfbeta + \beta_0)\geq0$ för alla $i=1,\dots,~N$, där $y_i$ är klasserna i klassificeringsproblemet, så får man en entydig definition av hyperplanet där vektorn $\bfbeta$ ''pekar mot`` klassen där $y_i=1$ och $\beta_0$ anger det signerade avståndet (med avseende på vart $\bfbeta$ pekar) från origo till hyperplanet.
\end{rem}
\begin{reas}
	De extra villkoren gör att man inte längre kan göra manipulationerna som påvisade icke-entydigheten. Om man sätter $\mathbf{x}=\bar{0}$ så får man med hjälp av sats \ref{thm:hyperplan} att avståndet från origo till planet är lika med $\frac{1}{\|\bfbeta\|}(\mathbf{x}^\intercal\bfbeta+\beta_0)=\beta_0$.
\end{reas}

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{example-image-a}
\caption{\label{fig:separatinghyperplane}20 datapunkter med ett separerande hyperplan (linje) där klassen $y=1$ har färgats blå och klassen $y=-1$ har färgats orange.}
\end{figure}

\begin{defi}
	Ett klassificeringsproblem kallas \textit{linjärt separabelt} om det existerar ett hyperplan $L=\sephyp$ som separerar mängderna.
\end{defi}

\begin{thm}
	För ett hyperplan $L=\entsephyp$ som separerar två klasser gäller att
	\begin{equation}
		y_i(\mathbf{x}_i^\intercal \bfbeta + \beta_0) > 0
	\end{equation}
	för alla $i = 1, \dots,~N$.
\end{thm}
\begin{proof}
	Ifall ett klassificeringsproblem är linjärt separabelt så ligger alla observationer $y_i$ på rätt sida av hyperplanet definierat genom $\mathbf{x}^\intercal \bfbeta + \beta_0$; eller så ligger alla observationer på fel sida av hyperplanet. Vilket betyder att ifall $y_i=1$ så är $\mathbf{x}_i^\intercal \bfbeta + \beta_0 > 0$ och om $y_i=-1$ så är $\mathbf{x}_i^\intercal \bfbeta + \beta_0 < 0$. Detta betyder att $y_i(\mathbf{x}_i^\intercal \bfbeta + \beta_0) > 0$. Ifall $\mathbf{x}_i^\intercal \bfbeta + \beta_0 = 0$ är problemet inte linjärt separabelt.
\end{proof}

\begin{ex}\label{exempel:mångahyperplan}
	Låt träningsdataparen vara $(\left[2,~2\right]^\intercal,~1),~(\left[1,~2\right]^\intercal,-1)$. Då är $$L_1=\{\mathbf{x}\in\mathbb{R}^2: \mathbf{x}^\intercal\begin{bmatrix}
	1\\
	0
	\end{bmatrix} - 1.5=0\}$$ och $$L_2=\{\mathbf{x}\in\mathbb{R}^2: \mathbf{x}^\intercal\begin{bmatrix}
	\sqrt{2}\\
	\sqrt{2}
	\end{bmatrix} - 3.5\sqrt{2}=0\}$$ två separerande hyperplan (linjer i detta fall).
\end{ex}
\begin{proof}
	För $L_1$: $$y_1(\mathbf{x}_1^\intercal\begin{bmatrix}
	1\\
	0\end{bmatrix}-1.5)=\left[2,~2\right]^\intercal\begin{bmatrix}
	1\\
	0\end{bmatrix}-1.5=0.5>0$$ och $$y_2(\mathbf{x}_2^\intercal\begin{bmatrix}
	1\\
	0\end{bmatrix}-1.5)=-1(\left[1,~2\right]^\intercal\begin{bmatrix}
	1\\
	0\end{bmatrix}-1.5)=(-1)(-0.5)=0.5>0.$$
	Och för $L_2$: $$y_1(\mathbf{x}_1^\intercal\begin{bmatrix}
	\sqrt{2}\\
	\sqrt{2}\end{bmatrix}-3.5\sqrt{2})=\left[2,~2\right]^\intercal\begin{bmatrix}
	\sqrt{2}\\
	\sqrt{2}\end{bmatrix}-3.5\sqrt{2}=0.5\sqrt{2}>0$$ och $$y_2(\mathbf{x}_2^\intercal\begin{bmatrix}
	\sqrt{2}\\
	\sqrt{2}\end{bmatrix}-3.5\sqrt{2})=-1(\left[1,~2\right]^\intercal\begin{bmatrix}
	\sqrt{2}\\
	\sqrt{2}\end{bmatrix}-3.5\sqrt{2})=(-1)(-0.5\sqrt{2})=0.5\sqrt{2}>0$$
\end{proof}

\begin{rem}
	Hyperplan kan konstrueras enkelt genom att man i $\mathbb{R}^n$ väljer $n$ stycken punkter $\mathbf{x}_i$ som man vill att planet ska gå igenom och sedan löser ekvationssystemet $X\bfbeta=-\bfbeta_0$ där $X$ är en matris där raderna består av punkterna $\mathbf{x}_i$, $i=1,\dots, n$ och $\bfbeta_0$ är en vektor av värdena $\beta_0$.
\end{rem}

Som syns i exempel \ref{exempel:mångahyperplan} så kan det finnas många separerande hyperplan ifall ett klassificeringsproblem är linjärt separabelt och frågan är ju då vilket av alla separerande hyperplan man borde välja.
%%%%	KANSKE REFERENS TILL VAPNIK 1996

\section{Optimala separerande hyperplan}
Inom statistiken finns många olika sätt att anpassa en modell till data och dessa sätt kan ofta visas vara ekvivalenta med något optimeringsproblem, till exempel maximum likelihood-metoden (ML-metoden) för linjär regression som kan visas vara ekvivalent med minstakvadratmetoden. %TODO% REFERENS 
Dessa optimeringsproblem kan oftast ändras genom att man lägger till eller tar bort termer i objektivfunktionen eller ändrar på kraven.

För separerande hyperplan kommer vi att behandla ett optimeringsproblem som är utformat så att det kortaste avståndet från hyperplanet till de närmaste träningspunkterna från vardera klass maximeras \cite{Vapnik96}. Med andra ord fås följande optimeringsproblem
\begin{equation}\label{opt:optimalmargin1}
\begin{aligned}
	& \operatornamewithlimits{max}_{\widehat{\bfbeta},\beta_0,\|\widehat{\bfbeta}\|=1} & & C\\
	& \text{så att} & & y_i(\mathbf{x}_i^\intercal\widehat{\bfbeta}+\beta_0)\geq C,\quad i=1,\dots,N
\end{aligned}
\end{equation}
där $C$ kallas marginalen och betecknar avståndet från hyperplanet till de närmaste punkterna.
\begin{rem}
	Ifall alla punkter är rätt klassificerade så anger $y_i(\mathbf{x}^\intercal_i\widehat{\bfbeta}+\beta_0)$ det absoluta avståndet mellan hyperplanet och punkten $\mathbf{x}_i$.
\end{rem}

Förhoppningen är här att man genom att välja det separerande hyperplan som befinner sig så långt som möjligt från båda klasserna hittar ett hyperplan som även generaliserar till ny data så bra som möjligt. Dessutom kommer vi också se att detta är ett sätt att unikt välja ett separerande hyperplan.

För att se att optimeringsproblemet (\ref{opt:optimalmargin1}) är \textit{konvext}, det vill säga har en unik lösning, måst vi skriva om det något. Vi börjar med att gör oss av med kravet $\|\widehat{\bfbeta}\|=1$ genom att byta ut kraven
\begin{equation*}
y_i(\mathbf{x}_i^\intercal\widehat{\bfbeta}+\beta_0)\geq C,\quad i=1,\dots,N
\end{equation*}
mot kraven
\begin{equation*}
y_i(\mathbf{x}_i^\intercal\frac{\bfbeta}{\|\bfbeta\|}+\frac{\beta_1}{\|\bfbeta\|}) = 
\frac{1}{\|\bfbeta\|}y_i(\mathbf{x}_i^\intercal\bfbeta+\beta_1)
 \geq C,\quad i=1,\dots,N
\end{equation*}
eller ekvivalent
\begin{equation*}
y_i(\mathbf{x}_i^\intercal\bfbeta+\beta_1)\geq C\|\bfbeta\|,\quad i=1,\dots,N.
\end{equation*}
där man alltså valt en av de andra representationerna för samma hyperplan genom att skala om $\widehat{\bfbeta}$ och $\beta_0$. Vidare kan $C$ elimineras genom att man väljer $C=\frac{1}{\|\bfbeta\|}$ och får
\begin{equation*}
y_i(\mathbf{x}_i^\intercal\bfbeta+\beta_1)\geq 1,\quad i=1,\dots,N
\end{equation*}
och eftersom att $C=\frac{1}{\|\bfbeta\|}$ ökar när $\|\bfbeta\|$ minskar är maximering av $C$ ekvivalent med minimering av $\|\bfbeta\|$ så vi får optimeringsproblemet
\begin{equation*}%\label{opt:optimalmargin2}
\begin{aligned}
& \operatornamewithlimits{min}_{\bfbeta,\beta_1} & & \|\bfbeta\|\\
& \text{så att} & & y_i(\mathbf{x}_i^\intercal\bfbeta+\beta_1)\geq 1,\quad i=1,\dots,N.
\end{aligned}
\end{equation*}
Därefter görs ännu en kvadratisk transformering av \textit{kostfunktionen} $\|\bfbeta\|$ alltså man noterar att $\operatornamewithlimits{argmin}_{\bfbeta,\beta_1} \|\bfbeta\|=\operatornamewithlimits{argmin}_{\bfbeta,\beta_1} \frac{1}{2}\|\bfbeta\|^2$. Med andra ord har vi ett optimeringsproblem av en kvadratisk funktion med linjära krav alltså ett konvext optimeringsproblem där lösningar existerar.

\begin{rem}
	För två vektorer $\mathbf{a}$ och $\mathbf{b}$ i $\mathbb{R}^n$ kan produkten $\mathbf{a}^\intercal\mathbf{b}$ uttryckas som den normala inre produkten $\langle \mathbf{a}, \mathbf{b} \rangle$ i $\mathbb{R}^n$. Detta kommer att komma till nytta i kapitel \ref{chap:hilbert} där konvexiteten för en utvidgning av det linjära problemet utforskas.
\end{rem}

Ovanstående resonemang ger alltså ett bevis för sats \ref{thm:primallinearproblem}.
\begin{thm}\label{thm:primallinearproblem}
	Låt $\widehat{\bfbeta},~\bfbeta \in \mathbb{R}^p$ och $\beta_0,~\beta_1 \in \mathbb{R}$. Då är optimeringsproblemet
	\begin{equation*}
	\begin{aligned}
	& \operatornamewithlimits{max}_{\widehat{\bfbeta},\beta_0,\|\widehat{\bfbeta}\|=1} & & C\\
	& \text{så att} & & y_i(\mathbf{x}_i^\intercal\widehat{\bfbeta}+\beta_0)\geq C,\quad i=1,\dots,N
	\end{aligned}
	\end{equation*}
	konvext och ekvivalent med optimeringsproblemet
	\begin{equation*}
	\begin{aligned}
	& \operatornamewithlimits{min}_{\bfbeta,\beta_1} & & \frac{1}{2}\|\bfbeta\|^2\\
	& \text{så att} & & y_i(\mathbf{x}_i^\intercal\bfbeta+\beta_1)\geq 1,\quad i=1,\dots,N
	\end{aligned}
	\end{equation*}
	ifall träningsdatat $(\mathbf{x}_i,y_i)$ är sådant att klassificeringsproblemet är linjärt separabelt.
\end{thm}
Vidare ger observationen ovan ett till ekvivalent optimeringsproblem:
\begin{cor}\label{cor:inreproduktoptimering}
	Optimeringsproblemen i sats \ref{thm:primallinearproblem} är ekvivalenta med optimeringsproblemet
	\begin{equation*}
	\begin{aligned}
	& \operatornamewithlimits{min}_{\bfbeta,\beta_1} & & \frac{1}{2}\langle \bfbeta, \bfbeta \rangle\\
	& \text{så att} & & y_i(\langle \mathbf{x}_i,\bfbeta\rangle+\beta_1)\geq 1,\quad i=1,\dots,N
	\end{aligned}
	\end{equation*}
	för samma krav.
\end{cor}
\begin{proof}
	Normen $\|\bfbeta\|=(\bfbeta^\intercal\bfbeta)^{\frac{1}{2}}$ kan uttryckas som $\langle \bfbeta, \bfbeta \rangle^{\frac{1}{2}}$ alltså är $\frac{1}{2} \|\bfbeta\|^2=\frac{1}{2}\langle \bfbeta, \bfbeta \rangle$. Resten följer från observationen.
\end{proof}
Framställningen i korollarium \ref{cor:inreproduktoptimering} används i många källor, bland annat i den ursprungliga framställningen för stödvektormaskinen %TODO
och är en av de mer generella framställningarna för optimeringsproblemet som stödvektormaskinen bygger på. %TODO Ha kvar dethär?

\subsection{Primala och duala problem}

För att hitta alla extrempunkter till ett optimeringsproblem, det vill säga lösa ett konvext optimeringsproblem, används Lagrangemultiplikatorer.
Den primala Lagrangefunktionen $L_P$ för optimeringsproblemet
\begin{equation*}
\begin{aligned}
& \operatornamewithlimits{min}_{\bfbeta,\beta_1} & & \frac{1}{2}\|\bfbeta\|^2\\
& \text{så att} & & y_i(\mathbf{x}_i^\intercal\bfbeta+\beta_1)\geq 1,\quad i=1,\dots,N
\end{aligned}
\end{equation*}
ges då av
\begin{equation}\label{eq:primallagrange}
L_P=\frac{1}{2}\|\bfbeta\|^2 - \sum_{i=1}^{N} \lambda_i\left(y_i \left(\mathbf{x}_i^\intercal\bfbeta + \beta_1\right)-1\right)
\end{equation}
som ska minimeras med avseende på $\bfbeta$ och $\beta_1$.

För att minimera $L_P$ sätter vi derivatorna med avseende på elementen $\left[\bfbeta\right]_j$ av $\bfbeta$ och $\beta_1$ till 0, det vill säga vi får följande relationer:
\begin{equation}\label{eq:derivprimal}
\begin{aligned}
	D_{ \left[\bfbeta\right]_j } (L_P) &= D_{ \left[\bfbeta\right]_j } \left( \frac{1}{2} \bfbeta^\intercal \bfbeta \right) &- &D_{ \left[\bfbeta\right]_j } \left( \sum_{i=1}^{N} \left( \lambda_i y_i \left( \mathbf{x}_i^\intercal\bfbeta \right) + \lambda_i y_i \beta_1 - \lambda_i \right)\right)\\
	&= D_{ \left[\bfbeta\right]_j } \left( \frac{1}{2} \sum_{k=1}^{p} \left[\bfbeta\right]_k^2 \right) &- &\sum_{i=1}^{N} D_{ \left[\bfbeta\right]_j } \left(  \lambda_i y_i \left(\sum_{k=1}^{p}\left[\mathbf{x}_i\right]_k\left[\bfbeta\right]_k \right) + \lambda_i y_i \beta_1-\lambda_i \right)\\
	&= [\bfbeta]_j &- &\sum_{i=1}^{N} D_{ \left[\bfbeta\right]_j } \left( \sum_{k=1}^{p} \lambda_i y_i \left[\mathbf{x}_i\right]_k\left[\bfbeta\right]_k \right) + 0\\
	&= [\bfbeta]_j &- &\sum_{i=1}^{N}\lambda_i y_i \left[ \mathbf{x}_i \right]_j
\end{aligned}
\end{equation}
där $j=1,\dots p$ och
\begin{equation*}
	D_{\beta_1}(L_P) = D_{\beta_1}\left( -\sum_{i=1}^{N} \lambda_i y_i \beta_1 \right) = -\sum_{i=1}^{N} \lambda_i y_i.
\end{equation*}
Vidare kan \ref{eq:derivprimal} skrivas om som derivatan med avseende på hela $\bfbeta$ eftersom att $ \left[ D_{ \bfbeta }(L_p) \right]_j = D_{\left[ \bfbeta \right]_j}(L_p) $. Efter att man tar i beaktande kraven att $ D_{ \bfbeta }(L_p) = \mathbf{0} $ och $ D_{\beta_1}(L_p)=0 $ fås följande krav:
\begin{align}\label{eq:krav1}
	\bfbeta &= \sum_{i=1}^{N} \lambda_i y_i \mathbf{x}_i\\
	0 &= \sum_{i=1}^{N} \lambda_i y_i.\label{eq:krav2}
\end{align}

Efter omskrivning av $\left\| \bfbeta \right\|^2$ som $\bfbeta^\intercal\bfbeta$ ger insättning av kraven \ref{eq:krav1} och \ref{eq:krav2} i $L_P$ följande duala problem

\begin{equation*}
\begin{aligned}
	L_D=&\frac{1}{2}\left(\sum_{i=1}^{N}\lambda_i y_i \mathbf{x}_i\right)^\intercal \left(\sum_{j=1}^{N}\lambda_j y_j \mathbf{x}_j\right)&\\
	&- \sum_{i=1}^{N}\lambda_i \left(y_i\left(\mathbf{x}_i^\intercal \left(\sum_{j=1}^{N} \lambda_j y_j \mathbf{x}_j\right) +\beta_1 \right) -1\right)&\\
	=& \frac{1}{2} \sum_{i=1}^{N} \lambda_i y_i \mathbf{x}_i^\intercal\left(\sum_{j=1}^{N} \lambda_j y_j \mathbf{x}_j\right) &\\
	&- \sum_{i=1}^{N}\lambda_i y_i \mathbf{x}_i^\intercal \left(\sum_{j=1}^{N} \lambda_j y_j \mathbf{x}_j\right) - \beta_1 \sum_{i=1}^{N} \lambda_i y_i  + \sum_{i=1}^{N} \lambda_i&\\
	=& -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_i \lambda_j y_i y_j \mathbf{x}_i^\intercal \mathbf{x}_j + \sum_{i=1}^{N} \lambda_i &\textstyle{\left(\sum\limits_{i=1}^{N}\lambda_iy_i = 0\right)}
\end{aligned}
\end{equation*}
som ska maximeras med avseende på $\lambda_i,~i=1,\dots,N,$ och kravet \begin{equation}\label{eq:krav3}
	\lambda_i\geq 0,~i=1,\dots,N.
\end{equation} Uträkningarna och kravet $\lambda_i\geq 0,~i=1,\dots,N,$ kan motiveras genom Karush-Kuhn-Tucker kraven för konvexa problem, det vill säga kraven \ref{eq:krav1}, \ref{eq:krav2} och \ref{eq:krav3} samt kravet
\begin{equation}\label{eq:krav4}
	\lambda_i\left( y_i\left( \mathbf{x}_i^\intercal \bfbeta + \beta_1 \right) -1 \right) = 0,~i=1,\dots.
\end{equation}
\begin{rem}
	Kraven \ref{eq:krav1} till \ref{eq:krav4} säger något om hurudan den optimala lösningen $\left(\bfbeta,\beta_1\right)$ måste vara;
	\begin{itemize}
		\item Krav \ref{eq:krav1} säger att vektorn $\bfbeta$ är en linjär kombination av vektorerna $y_i\mathbf{x}_i,~i=1,\dots,N$.
		\item Ifall $\lambda_i > 0$ så ger krav \ref{eq:krav4} att 
	\end{itemize}
\end{rem}

\section{Det oseparabla fallet}
Optimera med slack-variabler
\section{En enkel utvidgning med olinjära faktorer?}
Visa att problemet lösbart
\chapter{Hilbertrumteori, reproducerande kärnor}\label{chap:hilbert}
Varför utvidga fakoterna?
\section{Grundläggande teori}
Bevis av Mercers villkor för positivsemidefinita ekvationer/operatorer.
\section{SVM som exempel}
Något exempel. Introducera SVM för regression?

\bibliographystyle{plain}
\bibliography{bibliografi}
\end{document}          
